# ğŸš€ Pipeline Data Engineer Complet

## Architecture
```
[Sources de donnÃ©es] â†’ [ETL Python] â†’ [PostgreSQL + MongoDB] â†’ [API REST FastAPI]
       â†“                    â†“                   â†“                      â†“
  (CSV/API/JSON)     (Nettoyage/Transform)  (Stockage)          (Exposition)
                          â†“
                     [Logs + Monitoring]
```

## Stack Technique
- **Python 3.10+** â€” ETL, API
- **FastAPI** â€” API REST
- **PostgreSQL** â€” donnÃ©es structurÃ©es (relationnelles)
- **MongoDB** â€” donnÃ©es semi-structurÃ©es (logs, events)
- **SQLAlchemy** â€” ORM PostgreSQL
- **Alembic** â€” migrations DB
- **Loguru** â€” logging avancÃ©
- **Pandas** â€” transformation de donnÃ©es
- **Docker + Docker Compose** â€” orchestration

---

## âš™ï¸ Installation sur ta machine

### 1. PrÃ©requis
```bash
# Installer Python 3.10+
sudo apt update && sudo apt install python3 python3-pip python3-venv -y

# Installer PostgreSQL
sudo apt install postgresql postgresql-contrib -y
sudo systemctl start postgresql

# Installer MongoDB
curl -fsSL https://www.mongodb.org/static/pgp/server-7.0.asc | sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg --dearmor
echo "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list
sudo apt update && sudo apt install -y mongodb-org
sudo systemctl start mongod

# Docker (optionnel mais recommandÃ©)
sudo apt install docker.io docker-compose -y
sudo usermod -aG docker $USER
```

### 2. Environnement Python
```bash
cd data_pipeline
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Configuration PostgreSQL
```bash
sudo -u postgres psql
CREATE USER pipeline_user WITH PASSWORD 'pipeline_pass';
CREATE DATABASE pipeline_db OWNER pipeline_user;
GRANT ALL PRIVILEGES ON DATABASE pipeline_db TO pipeline_user;
\q
```

### 4. Copier et configurer le .env
```bash
cp config/.env.example .env
# Ã‰diter .env avec tes credentials
```

### 5. Migrations de base de donnÃ©es
```bash
alembic upgrade head
```

### 6. Lancer le pipeline
```bash
# ETL complet
python etl/main_pipeline.py

# API REST
uvicorn api.main:app --reload --port 8000
```

### Option Docker Compose (tout en un)
```bash
docker-compose up -d
```

---

## ğŸ“ Structure du Projet
```
data pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py       # Collecte des donnÃ©es
â”‚   â”œâ”€â”€ transform.py     # Nettoyage & transformation
â”‚   â”œâ”€â”€ load.py          # Chargement PostgreSQL + MongoDB
â”‚   â””â”€â”€ main_pipeline.py # Orchestrateur principal
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ postgres.py      # Connexion + modÃ¨les SQLAlchemy
â”‚   â”œâ”€â”€ mongodb.py       # Connexion + opÃ©rations MongoDB
â”‚   â””â”€â”€ migrations/      # Alembic migrations
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py          # App FastAPI
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ products.py  # Endpoints produits
â”‚   â”‚   â””â”€â”€ pipeline.py  # Endpoints pipeline status
â”‚   â””â”€â”€ schemas.py       # Pydantic models
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log     # Logs gÃ©nÃ©rÃ©s automatiquement
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_etl.py
â”‚   â””â”€â”€ test_api.py
â””â”€â”€ scripts/
    â”œâ”€â”€ seed_data.py     # GÃ©nÃ©rer des fausses donnÃ©es
    â””â”€â”€ health_check.py  # VÃ©rifier l'Ã©tat du systÃ¨me
```
